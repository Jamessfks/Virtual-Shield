# -*- coding: utf-8 -*-
"""Backend for Synthetic AI detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vZXv5x0z7KjfWy6dY6LNZ5Wy3Sp0vofA
"""

!pip install textdescriptives spacy pandas numpy scikit-learn tensorflow shap matplotlib seaborn
!python -m spacy download en_core_web_sm

!pip install --upgrade --force-reinstall numpy pandas

# Commented out IPython magic to ensure Python compatibility.
# %%time
# import pandas as pd
# import spacy
# import textdescriptives as td
# from sklearn.model_selection import train_test_split
# from sklearn.preprocessing import StandardScaler
# import tensorflow as tf
# from tensorflow.keras.models import Sequential
# from tensorflow.keras.layers import Dense, Dropout, Conv1D, Flatten, BatchNormalization
# from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
# import numpy as np

"""# Kaggle AI vs Human Dataset"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# import os  # Import the os module for file and directory operations
# 
# !pip install kagglehub
# import kagglehub
#

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # âœ… Define the path to the downloaded dataset
# dataset_path = kagglehub.dataset_download("shanegerami/ai-vs-human-text")
# 
# # âœ… Find the CSV file within the downloaded dataset.
# csv_file = None
# for filename in os.listdir(dataset_path):
#     if filename.endswith(".csv"):
#         csv_file = filename
#         break
# 
# if csv_file is None:
#     raise FileNotFoundError(f"No CSV file found in the dataset directory: {dataset_path}")
# 
# # âœ… Construct the full path to the CSV file
# csv_file_path = os.path.join(dataset_path, csv_file)
# 
# # âœ… Load the full dataset using the correct path
# df = pd.read_csv(csv_file_path)
# 
# # âœ… Ensure correct column names
# df.rename(columns={'generated': 'label'}, inplace=True)  # Rename 'generated' column to 'label'
# 
# # Load and balance 10000 AI + 10000 Human
# # Assuming 'AI_Human.csv' is within the downloaded dataset
# ai_human_csv_path = os.path.join(dataset_path, "AI_Human.csv")  # Construct the full path
# 
# # Check if the file exists before attempting to read it
# if os.path.exists(ai_human_csv_path):
#     df = pd.read_csv(ai_human_csv_path)
#     df_ai = df[df['generated'] == 1].sample(n=10000, random_state=42)
#     df_human = df[df['generated'] == 0].sample(n=10000, random_state=42)
#     df_balanced = pd.concat([df_ai, df_human]).sample(frac=1, random_state=42).reset_index(drop=True)
#     df_balanced.rename(columns={'generated': 'label'}, inplace=True)
# else:
#     raise FileNotFoundError(f"File not found: {ai_human_csv_path}") # Raise error if file doesn't exist
# 
# # Load spaCy model with textdescriptives
# nlp = spacy.load("en_core_web_sm")
# nlp.add_pipe("textdescriptives/all")
# docs = list(nlp.pipe(df_balanced["text"].astype(str)))
# df_features = td.extract_df(docs)
# df_final = pd.concat([df_features, df_balanced["label"]], axis=1)
# 
# # âœ… Check for and handle missing values before dropping rows
# print(f"Shape of df_final before dropna: {df_final.shape}")
# print(f"Number of missing values in each column:\n{df_final.isnull().sum()}")
# 
# # Instead of dropping all rows with missing values, consider imputation or dropping specific columns
# # For example, to drop columns with more than 50% missing values:
# threshold = 0.5  # Set your threshold here
# df_final = df_final.dropna(thresh=int(threshold * len(df_final)), axis=1)
# 
# # Or, to impute missing values with the mean of each column:
# # for column in df_final.columns:
# #     if df_final[column].isnull().any():
# #         df_final[column].fillna(df_final[column].mean(), inplace=True)
# 
# print(f"Shape of df_final after handling missing values: {df_final.shape}")
#

# Commented out IPython magic to ensure Python compatibility.
# %%time
# from sklearn.model_selection import train_test_split
# from sklearn.preprocessing import StandardScaler
# import numpy as np
# 
# # âœ… Step 1: Prepare features and labels
# X = df_final.drop(columns=["label", "text"]) if "text" in df_final.columns else df_final.drop(columns=["label"])
# y = df_final["label"]
# 
# # âœ… Step 2: First, split 80% for training, 20% temp (which we'll further split into val/test)
# X_train, X_temp, y_train, y_temp = train_test_split(
#     X, y, test_size=0.2, random_state=42, stratify=y
# )
# 
# # âœ… Step 3: Split the 20% temp into 50% val, 50% test â†’ each gets 10% of total data
# X_val, X_test, y_val, y_test = train_test_split(
#     X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp
# )
# 
# # âœ… Step 4: Normalize features using StandardScaler
# scaler = StandardScaler()
# X_train = scaler.fit_transform(X_train)
# X_val = scaler.transform(X_val)
# X_test = scaler.transform(X_test)
# 
# # âœ… Step 5: Reshape for Conv1D input (samples, timesteps, features)
# X_train = X_train[..., np.newaxis]
# X_val = X_val[..., np.newaxis]
# X_test = X_test[..., np.newaxis]
# 
# # âœ… Confirm sizes
# print(f"âœ… Training: {X_train.shape[0]} samples")
# print(f"âœ… Validation: {X_val.shape[0]} samples")
# print(f"âœ… Testing: {X_test.shape[0]} samples")
# 
#

# Commented out IPython magic to ensure Python compatibility.
# %%time
# import numpy as np
# 
# print("Any NaNs in X_train?", np.isnan(X_train).any())
# print("Any NaNs in y_train?", np.isnan(y_train).any())
# print("Max value in X_train:", np.max(X_train))
# print("Min value in X_train:", np.min(X_train))
#

# Commented out IPython magic to ensure Python compatibility.
# %%time
# import numpy as np
# 
# # Find rows without NaNs
# mask = ~np.isnan(X_train).any(axis=(1, 2))
# 
# # Filter data
# X_train = X_train[mask]
# y_train = y_train[mask]
# 
# # Do the same for validation set if needed
# mask_val = ~np.isnan(X_val).any(axis=(1, 2))
# X_val = X_val[mask_val]
# y_val = y_val[mask_val]
#

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # Show the cleaned training and validation set shapes
# print("After NaN removal:")
# print("X_train shape:", X_train.shape)
# print("y_train shape:", y_train.shape)
# print("X_val shape:", X_val.shape)
# print("y_val shape:", y_val.shape)
#

# Commented out IPython magic to ensure Python compatibility.
# %%time
# model = Sequential([
#     Conv1D(128, kernel_size=3, activation="relu", input_shape=(X_train.shape[1], 1)),
#     BatchNormalization(),
#     Flatten(),
#     Dense(256, activation="relu"),
#     Dropout(0.4),
#     Dense(128, activation="relu"),
#     Dropout(0.3),
#     Dense(64, activation="relu"),
#     Dropout(0.2),
#     Dense(1, activation="sigmoid")
# ])
# 
# model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005),
#               loss="binary_crossentropy",
#               metrics=["accuracy"])
# 
# early_stopping = EarlyStopping(monitor="val_loss", patience=10, restore_best_weights=True)
# reduce_lr = ReduceLROnPlateau(monitor="val_loss", factor=0.5, patience=3)
# 
# history = model.fit(X_train, y_train,
#                     validation_data=(X_val, y_val),
#                     epochs=150,
#                     batch_size=32,
#                     callbacks=[reduce_lr])
# 
# model.save("cnn_synthetic_natural_classifier.h5")

import os

file_path = "cnn_synthetic_natural_classifier.h5"
size_bytes = os.path.getsize(file_path)
size_kb = size_bytes / 1024
size_mb = size_kb / 1024

print(f"Model size: {size_bytes} bytes")
print(f"Model size: {size_kb:.2f} KB")
print(f"Model size: {size_mb:.2f} MB")

# Commented out IPython magic to ensure Python compatibility.
# %%time
# #Removing nan from test set
# mask = ~np.isnan(X_test).any(axis=(1, 2))
# 
# X_test = X_test[mask]
# y_test = y_test[mask]

# Commented out IPython magic to ensure Python compatibility.
# %%time
# loss, accuracy = model.evaluate(X_test, y_test)
# print(f"Test Accuracy: {accuracy * 100:.2f}%")
#

# Commented out IPython magic to ensure Python compatibility.
# %%time
# from sklearn.metrics import (
#     accuracy_score,
#     precision_score,
#     recall_score,
#     f1_score,
#     roc_auc_score,
#     confusion_matrix,
#     log_loss
# )
# import numpy as np
# 
# # Predict probabilities and classes
# y_pred_proba = model.predict(X_test)
# y_pred = (y_pred_proba > 0.5).astype("int32")
# 
# # Accuracy
# acc = accuracy_score(y_test, y_pred)
# 
# # Precision
# precision = precision_score(y_test, y_pred)
# 
# # Recall
# recall = recall_score(y_test, y_pred)
# 
# # F1 Score
# f1 = f1_score(y_test, y_pred)
# 
# # ROC-AUC
# roc_auc = roc_auc_score(y_test, y_pred_proba)
# 
# # Confusion Matrix
# conf_matrix = confusion_matrix(y_test, y_pred)
# 
# # Log Loss
# logloss = log_loss(y_test, y_pred_proba)
# 
# # Print all
# print(f"Accuracy: {acc:.4f}")
# print(f"Precision: {precision:.4f}")
# print(f"Recall: {recall:.4f}")
# print(f"F1 Score: {f1:.4f}")
# print(f"ROC AUC: {roc_auc:.4f}")
# print(f"Log Loss: {logloss:.4f}")
# print("Confusion Matrix:")
# print(conf_matrix)
#

"""## Validation"""

# Commented out IPython magic to ensure Python compatibility.
!git clone https://github.com/rsickle1/human-v-ai.git
# %cd human-v-ai


!pip install -r requirements.txt

import pandas as pd
df_cv = pd.read_csv("5000human_5000machine.csv")


print(df_cv.shape)
print(df_cv.head())

df_cv = df_cv[['label', 'text']]
print(df_cv.head())

# Commented out IPython magic to ensure Python compatibility.
# %%time
# nlp = spacy.load("en_core_web_sm")
# nlp.add_pipe("textdescriptives/all")
# df_balanced = df_cv
# docs = list(nlp.pipe(df_balanced["text"].astype(str)))
# 
# df_features = td.extract_df(docs)
# 
# df_final = pd.concat([df_features, df_balanced["label"]], axis=1)

print(df_final.shape)
print(df_final.head())

threshold = 0.5  # Set your threshold here
df_final = df_final.dropna(thresh=int(threshold * len(df_final)), axis=1)

print(df_final.shape)
print(df_final.head())

X_V = df_final.drop(columns=["label", "text"]) if "text" in df_final.columns else df_final.drop(columns=["label"])
y_V = df_final["label"]

print(X_V.shape)
print(X_V.head())

print(y_V.shape)
print(y_V.head())

X_train_V, X_temp_V, y_train_V, y_temp_V = train_test_split(
    X_V, y_V, test_size=0.2, random_state=42, stratify=y_V
)

X_train_V = scaler.fit_transform(X_train_V)

X_train_V = X_train_V[..., np.newaxis]

print(f"âœ… Test A: {X_train_V.shape[0]} samples")

mask_val = ~np.isnan(X_train_V).any(axis=(1, 2))
X_train_V = X_train_V[mask_val]
y_train_V = y_train_V[mask_val]

loss, accuracy = model.evaluate(X_train_V, y_train_V)
print(f"Test A Accuracy: {accuracy * 100:.2f}%")

X_test_A, X_test_B, y_test_A, y_test_B = train_test_split(
    X_temp_V, y_temp_V, test_size=0.5, random_state=42, stratify=y_temp_V
)

X_test_A = scaler.fit_transform(X_test_A)
X_test_B = scaler.transform(X_test_B)

X_test_A = X_test_A[..., np.newaxis]
X_test_B = X_test_B[..., np.newaxis]

print(f"âœ… Test A: {X_test_A.shape[0]} samples")
print(f"âœ… Test B: {X_test_B.shape[0]} samples")

mask_val = ~np.isnan(X_test_A).any(axis=(1, 2))
X_test_A = X_test_A[mask_val]
y_test_A = y_test_A[mask_val]

mask_val = ~np.isnan(X_test_B).any(axis=(1, 2))
X_test_B = X_test_B[mask_val]
y_test_B = y_test_B[mask_val]

loss, accuracy = model.evaluate(X_test_A, y_test_A)
print(f"Test A Accuracy: {accuracy * 100:.2f}%")

loss, accuracy = model.evaluate(X_test_B, y_test_B)
print(f"Test B Accuracy: {accuracy * 100:.2f}%")

"""# New Section"""

import matplotlib.pyplot as plt
import seaborn as sns

# Plot Confusion Matrix
plt.figure(figsize=(6, 5))
sns.heatmap(
    conf_matrix,
    annot=True,
    fmt="d",
    cmap="Blues",
    xticklabels=[0, 1],
    yticklabels=[0, 1],
    annot_kws={"size": 18}  # <-- Increase the font size here
)

plt.xlabel('Predicted Label', fontsize=14)
plt.ylabel('True Label', fontsize=14)
plt.title('Neural Network Confusion Matrix', fontsize=16)
plt.show()

"""# linguistic analysis"""



!pip install nltk --quiet


import nltk
from nltk.corpus import gutenberg
from nltk.tokenize import sent_tokenize
import random
import matplotlib.pyplot as plt

nltk.download('punkt')
nltk.download('punkt_tab')
nltk.download('gutenberg')

from nltk.corpus import gutenberg

print("Available texts:")
print(gutenberg.fileids())

import nltk
import random
import numpy as np
import matplotlib.pyplot as plt
from nltk.corpus import gutenberg
from nltk.tokenize import sent_tokenize

# Ensure resources are downloaded
nltk.download("punkt")
nltk.download("gutenberg")

# Your classify_text function (unchanged)
def classify_text(text: str) -> float:
    df = td.extract_metrics(text=text, lang="en", metrics=None)
    df_num = df.drop(columns=["text"], errors='ignore')
    df_aligned = df_num.reindex(columns=X.columns).fillna(0)
    scaled = scaler.transform(df_aligned)
    sample = scaled.reshape((1, scaled.shape[1], 1)).astype(np.float32)
    pred = model.predict(sample, verbose=0)[0][0]
    return pred

# --- Balanced selection: 4 books per century ---
century_books = {
    "16th": [
        "shakespeare-caesar.txt",
        "shakespeare-hamlet.txt",
        "shakespeare-macbeth.txt",
        "bible-kjv.txt"
    ],
    "17th": [
        "milton-paradise.txt",
        "blake-poems.txt",
        "edgeworth-parents.txt",
        "burgess-busterbrown.txt"
    ],
    "18th": [
        "austen-emma.txt",
        "austen-persuasion.txt",
        "austen-sense.txt",
        "bryant-stories.txt"
    ],
    "19th": [
        "carroll-alice.txt",
        "melville-moby_dick.txt",
        "whitman-leaves.txt",
        "chesterton-ball.txt"
    ],
    "20th": [
        "chesterton-brown.txt",
        "chesterton-thursday.txt",
        "burgess-busterbrown.txt"
    ]
}


samples_per_file = 40
sentences_per_sample = 25

centuries = []
accuracies = []

for century, files in century_books.items():
    correct = 0
    total = 0

    for fileid in files:
        try:
            raw = gutenberg.raw(fileid)
            sentences = sent_tokenize(raw)

            if len(sentences) < sentences_per_sample:
                continue
            random.shuffle(sentences)

            # Non-overlapping chunk sampling where possible
            max_start = len(sentences) - sentences_per_sample
            step = max(1, max_start // samples_per_file)

            for idx in range(0, min(samples_per_file * step, max_start), step):
                sample = " ".join(sentences[idx:idx + sentences_per_sample])

                try:
                    pred_score = classify_text(sample)
                    predicted_label = 0 if pred_score < 0.5 else 1
                    if predicted_label == 0:
                        correct += 1
                    total += 1
                except Exception as e:
                    print(f"[Score error in {fileid}] {e}")

        except Exception as e:
            print(f"[Load error in {fileid}] {e}")

    if total > 0:
        accuracy = correct / total
        centuries.append(century)
        accuracies.append(accuracy)
        print(f"{century} â€” Accuracy: {accuracy:.2%} ({correct}/{total})")
    else:
        print(f"{century} â€” No valid samples.")

# --- Plotting ---
plt.figure(figsize=(10, 6))
# Sort centuries chronologically
sorted_pairs = sorted(zip(centuries, accuracies), key=lambda x: int(x[0][:-2]))
sorted_centuries, sorted_accuracies = zip(*sorted_pairs)

plt.plot(sorted_centuries, sorted_accuracies, marker='o', linestyle='-')
plt.title("Model Accuracy in Predicting Human Text by Century (Balanced, Expanded Samples)")
plt.xlabel("Century")
plt.ylabel("Accuracy")
plt.ylim(0, 1.05)
plt.grid(True)
plt.tight_layout()
plt.show()

import nltk
import random
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from nltk.corpus import gutenberg
from nltk.tokenize import sent_tokenize

# Ensure resources are downloaded
nltk.download("punkt")
nltk.download("gutenberg")

# Your classify_text function
def classify_text(text: str) -> float:
    df = td.extract_metrics(text=text, lang="en", metrics=None)
    df_num = df.drop(columns=["text"], errors='ignore')
    df_aligned = df_num.reindex(columns=X.columns).fillna(0)
    scaled = scaler.transform(df_aligned)
    sample = scaled.reshape((1, scaled.shape[1], 1)).astype(np.float32)
    pred = model.predict(sample, verbose=0)[0][0]
    return pred

# Books grouped by century (from available Gutenberg corpus)
century_books = {
    "16th": [
        "shakespeare-caesar.txt",
        "shakespeare-hamlet.txt",
        "shakespeare-macbeth.txt"
    ],
    "17th": [
        "milton-paradise.txt"
    ],
    "18th": [
        "edgeworth-parents.txt"
    ],
    "19th": [
        "austen-emma.txt",
        "austen-persuasion.txt",
        "austen-sense.txt",
        "carroll-alice.txt",
        "melville-moby_dick.txt",
        "whitman-leaves.txt",
        "blake-poems.txt"
    ],
    "20th": [
        "burgess-busterbrown.txt",
        "chesterton-ball.txt",
        "chesterton-brown.txt",
        "chesterton-thursday.txt",
        "bryant-stories.txt"
    ]
}

samples_per_file = 20
sentences_per_sample = 20

centuries = []
accuracies = []

for century, files in century_books.items():
    correct = 0
    total = 0

    for fileid in files:
        try:
            raw = gutenberg.raw(fileid)
            sentences = sent_tokenize(raw)
            if len(sentences) < sentences_per_sample:
                continue
            random.shuffle(sentences)

            for _ in range(samples_per_file):
                start = random.randint(0, len(sentences) - sentences_per_sample)
                sample = " ".join(sentences[start:start + sentences_per_sample])

                try:
                    pred_score = classify_text(sample)
                    predicted_label = 0 if pred_score < 0.5 else 1  # 0 = human, 1 = AI
                    if predicted_label == 0:
                        correct += 1
                    total += 1
                except Exception as e:
                    print(f"[Score error in {fileid}] {e}")

        except Exception as e:
            print(f"[Load error in {fileid}] {e}")

    if total > 0:
        accuracy = correct / total
        centuries.append(century)
        accuracies.append(accuracy)
        print(f"{century} â€” Accuracy: {accuracy:.2%} ({correct}/{total})")
    else:
        print(f"{century} â€” No valid samples.")

# Plotting
plt.figure(figsize=(10, 6))
plt.plot(centuries, accuracies, marker='o', linestyle='-')
plt.title("Model Accuracy in Predicting Human Text by Century")
plt.xlabel("Century")
plt.ylabel("Accuracy")
plt.ylim(0, 1.05)
plt.grid(True)
plt.tight_layout()
plt.show()

import nltk
nltk.download('punkt')
nltk.download('punkt_tab')
from nltk.tokenize import sent_tokenize
import random


century_map = {
    'shakespeare-hamlet.txt': '16th',
    'milton-paradise.txt': '17th',
    'blake-poems.txt': '18th',
    'austen-emma.txt': '19th',
    'austen-persuasion.txt': '19th',
    'austen-sense.txt': '19th',
    'bryant-stories.txt': '19th',
    'carroll-alice.txt': '19th',
    'melville-moby_dick.txt': '19th',
    'whitman-leaves.txt': '19th',
    'burgess-busterbrown.txt': '20th',
    'chesterton-ball.txt': '20th',
    'chesterton-brown.txt': '20th',
    'chesterton-thursday.txt': '20th'
}

literary_samples = {}

for fileid in gutenberg.fileids():
    century = century_map.get(fileid)
    if century:
        raw = gutenberg.raw(fileid)
        sentences = sent_tokenize(raw)
        random.shuffle(sentences)
        sample = " ".join(sentences[:30])  # 30 random sentences
        literary_samples.setdefault(century, []).append(sample)

import nltk
from nltk.corpus import gutenberg
from nltk.tokenize import sent_tokenize
import random

nltk.download("punkt")
nltk.download("gutenberg")
century_books = {
    "15th": ["caxton-blanchard.txt"],        # William Caxton (1475)
    "16th": ["shakespeare-caesar.txt"],      # Shakespeare (1599)
    "17th": ["milton-paradise.txt"],         # John Milton (1667)
    "18th": ["austen-emma.txt"],             # Austen (1815, early 19th, used as substitute)
    "19th": ["bronte-jane.txt"],             # BrontÃ« (1847)
    "20th": ["chesterton-thursday.txt"],     # G.K. Chesterton (1908)
    "21st": []
}


for century, files in century_books.items():
    print(f"\n=== {century} Century ===")
    for fileid in files:
        print(f"\nFile: {fileid}")
        try:
            raw = gutenberg.raw(fileid)
            sentences = sent_tokenize(raw)
            random.shuffle(sentences)
            sample_sentences = sentences[:5]
            for i, sentence in enumerate(sample_sentences, 1):
                print(f"{i}. {sentence[:200].strip()}...\n")
        except Exception as e:
            print(f"Could not load {fileid}: {e}")

def classify_text(text: str) -> float:
    """
    Returns modelâ€™s raw score (0 â‰ˆ human, 1 â‰ˆ AI) for one piece of text.
    """

    df = td.extract_metrics(text=text, lang="en", metrics=None)

    df_num = df.drop(columns=["text"], errors='ignore')
    df_aligned = df_num.reindex(columns=X.columns).fillna(0)

    scaled = scaler.transform(df_aligned)
    sample = scaled.reshape((1, scaled.shape[1], 1)).astype(np.float32)

    pred = model.predict(sample, verbose=0)[0][0]
    return pred

import nltk
import random
import numpy as np
import matplotlib.pyplot as plt
from nltk.corpus import gutenberg
from nltk.tokenize import sent_tokenize


nltk.download("punkt")
nltk.download("gutenberg")

century_books = {
    "15th": ["caxton-blanchard.txt"],
    "16th": ["shakespeare-caesar.txt"],
    "17th": ["milton-paradise.txt"],
    "18th": ["austen-emma.txt"],
    "19th": ["bronte-jane.txt"],
    "20th": ["chesterton-thursday.txt"],
    "21st": []
}


samples_per_century = 10
sentences_per_sample = 5

centuries = []
avg_scores = []

for century, files in century_books.items():
    all_scores = []

    for fileid in files:
        try:
            raw = gutenberg.raw(fileid)
            sentences = sent_tokenize(raw)
            random.shuffle(sentences)


            for i in range(samples_per_century):
                start_idx = random.randint(0, len(sentences) - sentences_per_sample)
                sample_sentences = sentences[start_idx:start_idx + sentences_per_sample]
                sample_text = " ".join(sample_sentences)

                try:
                    score = classify_text(sample_text)
                    all_scores.append(score)
                except Exception as e:
                    print(f"Error scoring sample from {fileid}: {e}")
        except Exception as e:
            print(f"Could not load {fileid}: {e}")

    if all_scores:
        average_score = np.mean(all_scores)
        centuries.append(century)
        avg_scores.append(average_score)
        print(f"{century} century â†’ {len(all_scores)} samples â†’ Avg score: {average_score:.4f}")
    else:
        print(f"No valid samples for {century}")


plt.figure(figsize=(10, 6))
plt.plot(centuries, avg_scores, marker='o', linestyle='-')
plt.title("Modelâ€™s AI Probability Score vs. Literary Century")
plt.xlabel("Century")
plt.ylabel("Avg Prediction Score (0=Human, 1=AI)")
plt.ylim(0, 1)
plt.grid(True)
plt.tight_layout()
plt.show()

import nltk
import random
import numpy as np
import matplotlib.pyplot as plt
from nltk.corpus import gutenberg
from nltk.tokenize import sent_tokenize

nltk.download("punkt")
nltk.download("gutenberg")

century_books = {
    "15th": ["caxton-blanchard.txt"],
    "16th": ["shakespeare-caesar.txt"],
    "17th": ["milton-paradise.txt"],
    "18th": ["austen-emma.txt"],
    "19th": ["bronte-jane.txt"],
    "20th": ["chesterton-thursday.txt"],
    "21st": []
}


samples_per_century = 10
sentences_per_sample = 10

centuries = []
accuracies = []

for century, files in century_books.items():
    correct = 0
    total = 0

    for fileid in files:
        try:
            raw = gutenberg.raw(fileid)
            sentences = sent_tokenize(raw)
            random.shuffle(sentences)

            for i in range(samples_per_century):
                start_idx = random.randint(0, len(sentences) - sentences_per_sample)
                sample = " ".join(sentences[start_idx:start_idx + sentences_per_sample])

                try:
                    pred_score = classify_text(sample)
                    predicted_label = 0 if pred_score < 0.5 else 1  # 0 = human, 1 = AI

                    if predicted_label == 0:
                        correct += 1
                    total += 1
                except Exception as e:
                    print(f"Scoring error in {fileid}: {e}")

        except Exception as e:
            print(f"Could not load {fileid}: {e}")

    if total > 0:
        accuracy = correct / total
        centuries.append(century)
        accuracies.append(accuracy)
        print(f"{century} century â€” Accuracy: {accuracy:.2%} ({correct}/{total})")
    else:
        print(f"No valid samples for {century}")

plt.figure(figsize=(10, 6))
plt.plot(centuries, accuracies, marker='o', linestyle='-')
plt.title("Model Accuracy in Predicting Human Text vs Century")
plt.xlabel("Century")
plt.ylabel("Accuracy")
plt.ylim(0, 1.05)
plt.grid(True)
plt.tight_layout()
plt.show()

import nltk
import random
import numpy as np
import matplotlib.pyplot as plt
from nltk.corpus import gutenberg
from nltk.tokenize import sent_tokenize

nltk.download("punkt")
nltk.download("gutenberg")

century_books = {
    "15th": ["caxton-blanchard.txt"],
    "16th": ["shakespeare-caesar.txt"],
    "17th": ["milton-paradise.txt"],
    "18th": ["austen-emma.txt"],
    "19th": ["bronte-jane.txt"],
    "20th": ["chesterton-thursday.txt"],
    "21st": []
}


samples_per_century = 20
sentences_per_sample = 20

centuries = []
accuracies = []

for century, files in century_books.items():
    correct = 0
    total = 0

    for fileid in files:
        try:
            raw = gutenberg.raw(fileid)
            sentences = sent_tokenize(raw)
            random.shuffle(sentences)

            for i in range(samples_per_century):
                start_idx = random.randint(0, len(sentences) - sentences_per_sample)
                sample = " ".join(sentences[start_idx:start_idx + sentences_per_sample])

                try:
                    pred_score = classify_text(sample)
                    predicted_label = 0 if pred_score < 0.5 else 1  # 0 = human, 1 = AI

                    if predicted_label == 0:
                        correct += 1
                    total += 1
                except Exception as e:
                    print(f"Scoring error in {fileid}: {e}")

        except Exception as e:
            print(f"Could not load {fileid}: {e}")

    if total > 0:
        accuracy = correct / total
        centuries.append(century)
        accuracies.append(accuracy)
        print(f"{century} century â€” Accuracy: {accuracy:.2%} ({correct}/{total})")
    else:
        print(f"No valid samples for {century}")

plt.figure(figsize=(10, 6))
plt.plot(centuries, accuracies, marker='o', linestyle='-')
plt.title("Model Accuracy in Predicting Human Text vs Century")
plt.xlabel("Century")
plt.ylabel("Accuracy")
plt.ylim(0, 1.05)
plt.grid(True)
plt.tight_layout()
plt.show()

import nltk
import random
import numpy as np
import matplotlib.pyplot as plt
from nltk.corpus import gutenberg
from nltk.tokenize import sent_tokenize

nltk.download("punkt")
nltk.download("gutenberg")

century_books = {
    "15th": ["caxton-blanchard.txt"],
    "16th": ["shakespeare-caesar.txt"],
    "17th": ["milton-paradise.txt"],
    "18th": ["austen-emma.txt"],
    "19th": ["bronte-jane.txt"],
    "20th": ["chesterton-thursday.txt"],
    "21st": []
}


samples_per_century = 10
sentences_per_sample = 5

centuries = []
accuracies = []

for century, files in century_books.items():
    correct = 0
    total = 0

    for fileid in files:
        try:
            raw = gutenberg.raw(fileid)
            sentences = sent_tokenize(raw)
            random.shuffle(sentences)

            for i in range(samples_per_century):
                start_idx = random.randint(0, len(sentences) - sentences_per_sample)
                sample = " ".join(sentences[start_idx:start_idx + sentences_per_sample])

                try:
                    pred_score = classify_text(sample)
                    predicted_label = 0 if pred_score < 0.5 else 1  # 0 = human, 1 = AI

                    if predicted_label == 0:
                        correct += 1
                    total += 1
                except Exception as e:
                    print(f"Scoring error in {fileid}: {e}")

        except Exception as e:
            print(f"Could not load {fileid}: {e}")

    if total > 0:
        accuracy = correct / total
        centuries.append(century)
        accuracies.append(accuracy)
        print(f"{century} century â€” Accuracy: {accuracy:.2%} ({correct}/{total})")
    else:
        print(f"No valid samples for {century}")

plt.figure(figsize=(10, 6))
plt.plot(centuries, accuracies, marker='o', linestyle='-')
plt.title("Model Accuracy in Predicting Human Text vs Century")
plt.xlabel("Century")
plt.ylabel("Accuracy")
plt.ylim(0, 1.05)
plt.grid(True)
plt.tight_layout()
plt.show()

import nltk
from nltk.corpus import gutenberg
from nltk.tokenize import sent_tokenize
import random

nltk.download('punkt')
nltk.download('gutenberg')

# Updated map of books to centuries (no Bible, no repeats)
century_map = {
    'shakespeare-hamlet.txt': '16th',
    'marlowe-doctor-faustus.txt': '16th',

    'milton-paradise.txt': '17th',

    'blake-poems.txt': '18th',
    'edgeworth-parents.txt': '18th',

    'austen-emma.txt': '19th',
    'austen-persuasion.txt': '19th',
    'austen-sense.txt': '19th',
    'bryant-stories.txt': '19th',
    'carroll-alice.txt': '19th',
    'melville-moby_dick.txt': '19th',
    'whitman-leaves.txt': '19th',

    'burgess-busterbrown.txt': '20th',
    'chesterton-ball.txt': '20th',
    'chesterton-brown.txt': '20th',
    'chesterton-thursday.txt': '20th'
}

literary_samples = {}

for fileid in gutenberg.fileids():
    century = century_map.get(fileid)
    if century:
        try:
            raw = gutenberg.raw(fileid)
            sentences = sent_tokenize(raw)
            random.shuffle(sentences)
            sample = " ".join(sentences[:30])  # 30 random sentences per text
            literary_samples.setdefault(century, []).append(sample)
        except Exception as e:
            print(f"Error processing {fileid}: {e}")

# Optional: show count of samples per century
for century, samples in literary_samples.items():
    print(f"{century} Century â†’ {len(samples)} samples")

# Optional: preview first 2 samples of each century
for century, samples in literary_samples.items():
    print(f"\n=== {century} Century ===")
    for i, sample in enumerate(samples[:2], 1):
        print(f"Sample {i}:\n{sample[:500]}...\n")

import nltk
from nltk.corpus import gutenberg
from nltk.tokenize import sent_tokenize
import random

nltk.download("punkt")
nltk.download("gutenberg")

# Selected different books from gutenberg corpus, by approximate century
century_books = {
    "16th": ["marlowe-doctor-faustus.txt"],         # Christopher Marlowe (c. 1592)
    "17th": ["shakespeare-hamlet.txt"],             # William Shakespeare (1603, borderline 17th)
    "18th": ["edgeworth-parents.txt"],              # Maria Edgeworth (~1796)
    "19th": ["melville-moby_dick.txt"],             # Herman Melville (1851)
    "20th": ["'burgess-busterbrown.txt'"],
}

for century, files in century_books.items():
    print(f"\n=== {century} Century ===")
    for fileid in files:
        print(f"\nFile: {fileid}")
        try:
            raw = gutenberg.raw(fileid)
            sentences = sent_tokenize(raw)
            random.shuffle(sentences)
            sample_sentences = sentences[:5]
            for i, sentence in enumerate(sample_sentences, 1):
                print(f"{i}. {sentence[:200].strip()}...\n")
        except Exception as e:
            print(f"Could not load {fileid}: {e}")

import nltk
import random
import numpy as np
import matplotlib.pyplot as plt
from nltk.corpus import gutenberg
from nltk.tokenize import sent_tokenize

# Assume these are defined in your environment:
# from your_model_file import model, scaler, X, td

nltk.download("punkt")
nltk.download("gutenberg")

def classify_text(text: str) -> float:
    """
    Returns modelâ€™s raw score (0 â‰ˆ human, 1 â‰ˆ AI) for one piece of text.
    """
    df = td.extract_metrics(text=text, lang="en", metrics=None)

    df_num = df.drop(columns=["text"], errors='ignore')
    df_aligned = df_num.reindex(columns=X.columns).fillna(0)

    scaled = scaler.transform(df_aligned)
    sample = scaled.reshape((1, scaled.shape[1], 1)).astype(np.float32)

    pred = model.predict(sample, verbose=0)[0][0]
    return pred

century_books = {
    "16th": ["marlowe-doctor-faustus.txt"],
    "17th": ["shakespeare-hamlet.txt"],
    "18th": ["edgeworth-parents.txt"],
    "19th": ["melville-moby_dick.txt"],
    "20th": ["burgess-busterbrown.txt"],
}

samples_per_book = 20
sentences_per_sample = 20

centuries = []
accuracies = []

for century, files in century_books.items():
    correct = 0
    total = 0

    for fileid in files:
        try:
            raw = gutenberg.raw(fileid)
            sentences = sent_tokenize(raw)
            random.shuffle(sentences)

            for _ in range(samples_per_book):
                if len(sentences) < sentences_per_sample:
                    continue
                start_idx = random.randint(0, len(sentences) - sentences_per_sample)
                sample = " ".join(sentences[start_idx:start_idx + sentences_per_sample])

                try:
                    pred_score = classify_text(sample)
                    predicted_label = 0 if pred_score < 0.5 else 1  # 0 = human, 1 = AI

                    if predicted_label == 0:
                        correct += 1
                    total += 1
                except Exception as e:
                    print(f"Scoring error in {fileid}: {e}")

        except Exception as e:
            print(f"Could not load {fileid}: {e}")

    if total > 0:
        accuracy = correct / total
        centuries.append(century)
        accuracies.append(accuracy)
        print(f"{century} century â€” Accuracy: {accuracy:.2%} ({correct}/{total})")
    else:
        print(f"No valid samples for {century}")

plt.figure(figsize=(10, 6))
plt.plot(centuries, accuracies, marker='o', linestyle='-')
plt.title("Model Accuracy in Predicting Human Text vs Century")
plt.xlabel("Century")
plt.ylabel("Accuracy (Predicted as Human)")
plt.ylim(0, 1.05)
plt.grid(True)
plt.tight_layout()
plt.show()

import nltk
import os
import random
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from nltk.corpus import gutenberg
from nltk.tokenize import sent_tokenize

nltk.download("punkt")
nltk.download("gutenberg")

# === Your classifier (already trained on Kaggle AI vs Human) ===
def classify_text(text: str) -> float:
    df = td.extract_metrics(text=text, lang="en", metrics=None)
    df_num = df.drop(columns=["text"], errors='ignore')
    df_aligned = df_num.reindex(columns=X.columns).fillna(0)
    scaled = scaler.transform(df_aligned)
    sample = scaled.reshape((1, scaled.shape[1], 1)).astype(np.float32)
    return model.predict(sample, verbose=0)[0][0]

# === Century â†’ List of book files ===
century_books = {
    "16th": [
        "marlowe-doctor-faustus.txt",
    ],
    "17th": [
        "shakespeare-hamlet.txt",
        "shakespeare-caesar.txt",
        "shakespeare-macbeth.txt",
    ],
    "18th": [
        "edgeworth-parents.txt",
        "burney-evelina.txt",
        "edgeworth-harry.txt",
    ],
    "19th": [
        "austen-emma.txt",
        "bronte-jane.txt",
        "melville-moby_dick.txt",
    ],
    "20th": [
        "chesterton-thursday.txt",
        "burgess-busterbrown.txt",
    ]
}

samples_per_book = 30
sentences_per_sample = 20

results = []

# === Iterate through books ===
for century, books in century_books.items():
    for book in books:
        try:
            # Load text
            if century == "21st":
                with open(f"data/21st/{book}", "r", encoding="utf-8") as f:
                    raw = f.read()
            else:
                raw = gutenberg.raw(book)

            sentences = sent_tokenize(raw)
            if len(sentences) < sentences_per_sample:
                continue
            random.shuffle(sentences)

            for _ in range(samples_per_book):
                start = random.randint(0, len(sentences) - sentences_per_sample)
                sample = " ".join(sentences[start:start + sentences_per_sample])

                try:
                    score = classify_text(sample)
                    predicted_label = 0 if score < 0.5 else 1
                    results.append({
                        "century": century,
                        "book": book,
                        "score": score,
                        "predicted_label": predicted_label
                    })
                except Exception as e:
                    print(f"[Score error] {book}: {e}")

        except Exception as e:
            print(f"[Load error] {book}: {e}")

# === Convert to DataFrame ===
df = pd.DataFrame(results)

# === Accuracy and 95% CI per century ===
summary = []
for century, group in df.groupby("century"):
    total = len(group)
    correct = (group["predicted_label"] == 0).sum()
    acc = correct / total
    ci_half = 1.96 * np.sqrt(acc * (1 - acc) / total)
    summary.append({
        "century": century,
        "accuracy": acc,
        "ci_lower": acc - ci_half,
        "ci_upper": acc + ci_half,
        "n": total
    })

summary_df = pd.DataFrame(summary).sort_values("century")

# === Plot: Accuracy with CI ===
plt.figure(figsize=(10, 6))
plt.errorbar(
    summary_df["century"], summary_df["accuracy"],
    yerr=[summary_df["accuracy"] - summary_df["ci_lower"],
          summary_df["ci_upper"] - summary_df["accuracy"]],
    fmt='o-', capsize=5
)
plt.title("Classifier Accuracy on Human Texts by Century (with 95% CI)")
plt.xlabel("Century")
plt.ylabel("Accuracy (Predicted as Human)")
plt.ylim(0, 1.05)
plt.grid(True)
plt.tight_layout()
plt.show()

# === Plot: Score Distributions ===
plt.figure(figsize=(10, 6))
for century in sorted(df["century"].unique()):
    scores = df[df["century"] == century]["score"]
    plt.hist(scores, bins=20, alpha=0.5, label=century, density=True)
plt.axvline(0.5, color="k", linestyle="--", label="0.5 threshold")
plt.title("Prediction Score Distributions by Century")
plt.xlabel("Model Score (0 = human, 1 = AI)")
plt.ylabel("Density")
plt.legend()
plt.tight_layout()
plt.show()

"""# others"""

occumsRazorText = '''The standard error ofÂ  point estimation is the approximate standard deviation of a parameter of a population from the statistic of a sample.

If xÌ„ is the mean of sample and Î¼ be the actual population mean then xÌ„ is the point estimation of Î¼.

Â  Â  Â  Â  Â  Â  Â  Â  standard error = Ïƒ /Â âˆšn

Â where , Ïƒ = standard deviation of the population

Â  Â  Â  Â  Â  Â  Â  n =Â  sample size'''

df_occum = td.extract_metrics(text=occumsRazorText, lang="en", metrics=None)

# print("Data types of df_occum values:", df_occum.dtypes)

df_occum_numeric = df_occum.drop(columns=["text"])

print("Data types of df_occum values:", df_occum_numeric.dtypes)

occum_features = df_occum_numeric.iloc[:, :68].values  # Should be shape (1, 68)
occum_scaled = scaler.transform(occum_features)

df_occum_aligned = df_occum_numeric[X.columns]
df_occum_aligned_filled = df_occum_aligned.fillna(0)
occum_scaled = scaler.transform(df_occum_aligned_filled)

occum_reshaped = occum_scaled.reshape((1, 68, 1)).astype(np.float32)


prediction = model.predict(occum_reshaped)

print("Prediction (0 for Human, 1 for AI):", prediction[0][0])

print(prediction)

def classify_text(text: str) -> float:
    """
    Returns modelâ€™s raw score (0â€¯â‰ˆâ€¯human, 1â€¯â‰ˆâ€¯AI) for one piece of text.
    """
    # 1) feature extraction
    df = td.extract_metrics(text=text, lang="en", metrics=None)

    # 2) keep only numeric cols & align to training feature order
    df_num   = df.drop(columns=["text"])
    df_align = df_num.reindex(columns=X.columns).fillna(0)

    # 3) scale & reshape
    scaled   = scaler.transform(df_align)
    sample   = scaled.reshape((1, 68, 1)).astype(np.float32)

    # 4) predict
    pred = model.predict(sample, verbose=0)[0][0]
    return pred          # or (pred > 0.5).astype(int) for hard label

crime_paragraphs = [
    "On an exceptionally hot evening early in July a young man came out of the garret in which he lodged in S. Place and walked slowly, as though in hesitation, towards K. bridge.",  # opening of Partâ€¯Iâ€¯Ch.â€¯1
    "He had successfully avoided meeting his landlady on the staircase.  His garret was under the roof of a high, fiveâ€‘storied house and was more like a cupboard than a room.",            # immediately following
    "The young man was not used to crowds, and, as we have said before, he was shy and distrustful of himself, which made him avoid society of every sort.",                          # a little farther in the same chapter
    "The heat in the street was terrible: and the airlessness, the bustle and the plaster, scaffolding, bricks, dust and that special Petersburg stench, so familiar to all who are unable to get out of town in summerâ€”all worked painfully upon the young man's already overwrought nerves."                                                                                                                             # later in Ch.â€¯1
]


macbeth_paragraphs = [
    "When shall we three meet again\nIn thunder, lightning, or in rain?",
    "When the hurlyâ€‘burlyâ€™s done,\nWhen the battleâ€™s lost and won.",
    "That will be ere the set of sun.",
    "Fair is foul, and foul is fair:\nHover through the fog and filthy air."
]

for t in crime_paragraphs + macbeth_paragraphs:
    score = classify_text(t)
    print(f"{score:.3f}\n{t}\n")

kafka_paragraphs = [
    "You look for silence, but guess what? All you hear over and over and over is the voice of this omen. And sometimes this prophetic voice pushes a secret switch hidden deep inside your brain.",
    "Your heart is like a great river after a long spell of rain, spilling over its banks. All signposts that once stood on the ground are gone, inundated and carried away by that rush of water. And still the rain beats down on the surface of the river. Every time you see a flood like that on the news you tell yourself: That's it. That's my heart.",
    "Before running away from home I wash my hands and face, trim my nails, swab out my ears, and brush my teeth. I take my time, making sure my whole body's well scrubbed. Being really clean is sometimes the most important thing there is. I gaze carefully at my face in the mirror. Genes I'd gotten from my father and motherâ€”not that I have any recollection of what she looked likeâ€”created this face. I can do my best to not let any emotions show, keep my eyes from revealing anything, bulk up my muscles, but there's not much I can do about my looks. I'm stuck with my father's long, thick eyebrows and the deep lines between them. I could probably kill him if I wanted toâ€”I'm sure strong enoughâ€”and I can erase my mother from my memory. But there's no way to erase the DNA they passed down to me. If I wanted to drive that away I'd have to get rid of me.",
    "By the time I arrived several children had partially regained consciousness. Three or four of them, as I recall. But they weren't fully consciousâ€”sort of dizzily on all fours. The rest of the children were still collapsed. After a while some of the others began to come around, their bodies undulating like so many big worms. It was a very strange sight. The children had collapsed in an odd, flat, open space in the woods where it looked like all the trees had been neatly removed, with autumn sunlight shining down brightly. And here you had, in this spot or at the edges of it, sixteen elementary-school kids scattered about prostrate on the ground, some of them starting to move, some of them completely still. The whole thing reminded me of some weird avant-garde play."
]

for t in kafka_paragraphs:
    score = classify_text(t)
    print(f"{score:.3f}\n{t}\n")

ai_paper_paragraphs = [
    "This paper examines the evolution, architecture, and practical applications of AI agents from their early, rule-based incarnations to modern sophisticated systems that integrate large language models with dedicated modules for perception, planning, and tool use.",
    "Emphasizing both theoretical foundations and real-world deployments, the paper reviews key agent paradigms, discusses limitations of current evaluation benchmarks, and proposes a holistic evaluation framework that balances task effectiveness, efficiency, robustness, and safety.",
    "Applications across enterprise, personal assistance, and specialized domains are analyzed, with insights into future research directions for more resilient and adaptive AI agent systems.",
    "The paper reviews key agent paradigms, discusses limitations of current evaluation benchmarks, and proposes a holistic evaluation framework that balances task effectiveness, efficiency, robustness, and safety."
]

for t in ai_paper_paragraphs:
    score = classify_text(t)
    print(f"{score:.3f}\n{t}\n")

test_paragraphs = [
    "To access what one needs in the world today, many would think one needs a car.",
    "How often do you ride in a car? Do you drive a one or any other motor vehicle to work? The store?",
    "There are many advantages to limiting car usage in our community. Other countries such as France",
    "It's official: The electoral college is unfair, outdated, and irrational. "
]

for t in test_paragraphs:
    score = classify_text(t)
    print(f"{score:.3f}\n{t}\n")

AI_paragraphs = ["Fyodor Dostoevskyâ€™s Crime and Punishment is a profound exploration of morality, guilt, and redemption. The novel delves into the psyche of Raskolnikov, a man who justifies murder with philosophical reasoning, only to be tormented by his conscience. Dostoevsky masterfully portrays the psychological consequences of crime, showing that true punishment stems not just from law, but from internal moral reckoning. The story challenges readers to consider the complexity of human nature and the redemptive power of suffering, making it a timeless and thought-provoking work."];

for t in AI_paragraphs:
    score = classify_text(t)
    print(f"{score:.3f}\n{t}\n")

# Get original test text
# You need this to match test set order
df_balanced = df_balanced.reset_index(drop=True)
df_final = df_final.reset_index(drop=True)

# Use the same test split you used before
_, test_text = train_test_split(df_balanced, test_size=100, random_state=42, stratify=df_balanced["label"])
test_text = test_text.reset_index(drop=True)

# Predict on X_test
y_pred_prob = model.predict(X_test)
y_pred = (y_pred_prob > 0.5).astype(int).flatten()

# Compare predictions
results_df = pd.DataFrame({
    "Text": test_text["text"],
    "True Label": y_test.values,
    "Predicted Label": y_pred,
})

# Add a readable label and correctness
results_df["True Label Name"] = results_df["True Label"].map({0: "Human", 1: "AI"})
results_df["Predicted Label Name"] = results_df["Predicted Label"].map({0: "Human", 1: "AI"})
results_df["Correct"] = results_df["True Label"] == results_df["Predicted Label"]

# Show full column width in output
pd.set_option('display.max_colwidth', None)

# Show 10 examples clearly
for i in range(100):
    print(f"\nðŸ”¹ Sentence {i+1}:")
    print(f"ðŸ“ Text: {results_df.loc[i, 'Text']}")
    print(f"âœ… True Label: {results_df.loc[i, 'True Label Name']}")
    print(f"ðŸ¤– Predicted: {results_df.loc[i, 'Predicted Label Name']}")
    print(f"ðŸŽ¯ Correct? {'Yes' if results_df.loc[i, 'Correct'] else 'No'}")

# Optional: Save to CSV
results_df.to_csv("ai_vs_human_predictions.csv", index=False)

model_Normal = Sequential([
    Dense(256, activation="relu", input_shape=(X_train.shape[1],)),
    Dropout(0.4),
    Dense(128, activation="relu"),
    Dropout(0.3),
    Dense(64, activation="relu"),
    Dropout(0.2),
    Dense(1, activation="sigmoid")
])

model_Normal.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005),
              loss="binary_crossentropy",
              metrics=["accuracy"])

early_stopping = EarlyStopping(monitor="val_loss", patience=10, restore_best_weights=True)
reduce_lr = ReduceLROnPlateau(monitor="val_loss", factor=0.5, patience=3)

history = model_Normal.fit(X_train, y_train,
                    validation_data=(X_val, y_val),
                    epochs=150,
                    batch_size=32,
                    callbacks=[reduce_lr])

model_Normal.save("cnn_synthetic_natural_classifier.h5")

from sklearn.metrics import (
    accuracy_score,
    precision_score,
    recall_score,
    f1_score,
    roc_auc_score,
    confusion_matrix,
    log_loss
)
import numpy as np

# Predict probabilities and classes
y_pred_proba = model_Normal.predict(X_test)
y_pred = (y_pred_proba > 0.5).astype("int32")

# Accuracy
acc = accuracy_score(y_test, y_pred)

# Precision
precision = precision_score(y_test, y_pred)

# Recall
recall = recall_score(y_test, y_pred)

# F1 Score
f1 = f1_score(y_test, y_pred)

# ROC-AUC
roc_auc = roc_auc_score(y_test, y_pred_proba)

# Confusion Matrix
conf_matrix = confusion_matrix(y_test, y_pred)

# Log Loss
logloss = log_loss(y_test, y_pred_proba)

# Print all
print(f"Accuracy: {acc:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall: {recall:.4f}")
print(f"F1 Score: {f1:.4f}")
print(f"ROC AUC: {roc_auc:.4f}")
print(f"Log Loss: {logloss:.4f}")
print("Confusion Matrix:")
print(conf_matrix)

import pandas as pd

df = pd.read_csv("human-v-ai/human_ai_dataset.csv")
df.head()

!git clone https://github.com/rsickle1/human-v-ai.git
import os
import pandas as pd

dataset_path = "human-v-ai"
csv_file_path = os.path.join(dataset_path, "5000human_5000machine.csv")

df = pd.read_csv(csv_file_path)


df.rename(columns={"label": "label", "text": "text"}, inplace=True)
df["label"] = df["label"].map({"machine": 1, "human": 0})  # 1 = AI, 0 = Human

# âœ… STEP 4: Extract features with textdescriptives
import spacy
import textdescriptives as td

nlp = spacy.load("en_core_web_sm")
nlp.add_pipe("textdescriptives/all")

docs = list(nlp.pipe(df["text"].astype(str)))

# Ensure label alignment
df["label"] = df["label"].astype(int)
df_features = td.extract_df(docs)

# Filter out rows that were skipped during spaCy processing
if len(df_features) != len(df):
    print(f"âš ï¸ Warning: Length mismatch! df_features: {len(df_features)}, df: {len(df)}")
    df_features = df_features.reset_index(drop=True)
    df_labels = df["label"].reset_index(drop=True)
else:
    df_labels = df["label"]

# Concatenate features with labels
df_final = pd.concat([df_features, df_labels], axis=1)

# âœ… STEP 5: Handle missing values
print(f"Shape before dropna: {df_final.shape}")
df_final = df_final.dropna(thresh=int(0.5 * len(df_final)), axis=1)
print(f"Shape after dropna: {df_final.shape}")

# âœ… STEP 6: Split and scale data
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import numpy as np

X = df_final.drop(columns=["label"])
y = df_final["label"]

X_train, X_temp, y_train, y_temp = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)
X_val, X_test, y_val, y_test = train_test_split(
    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp
)

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_val = scaler.transform(X_val)
X_test = scaler.transform(X_test)

X_train = X_train[..., np.newaxis]
X_val = X_val[..., np.newaxis]
X_test = X_test[..., np.newaxis]

# Remove NaNs if any
mask_train = ~np.isnan(X_train).any(axis=(1, 2))
X_train, y_train = X_train[mask_train], y_train[mask_train]

mask_val = ~np.isnan(X_val).any(axis=(1, 2))
X_val, y_val = X_val[mask_val], y_val[mask_val]

# âœ… STEP 7: Build and train model
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, BatchNormalization, Flatten, Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau

model = Sequential([
    Conv1D(128, kernel_size=3, activation="relu", input_shape=(X_train.shape[1], 1)),
    BatchNormalization(),
    Flatten(),
    Dense(256, activation="relu"),
    Dropout(0.4),
    Dense(128, activation="relu"),
    Dropout(0.3),
    Dense(64, activation="relu"),
    Dropout(0.2),
    Dense(1, activation="sigmoid")
])

model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005),
              loss="binary_crossentropy",
              metrics=["accuracy"])

early_stopping = EarlyStopping(monitor="val_loss", patience=10, restore_best_weights=True)
reduce_lr = ReduceLROnPlateau(monitor="val_loss", factor=0.5, patience=3)

history = model.fit(X_train, y_train,
                    validation_data=(X_val, y_val),
                    epochs=150,
                    batch_size=32,
                    callbacks=[reduce_lr, early_stopping])

# âœ… STEP 8: Save model
model.save("cnn_human_vs_ai_classifier.h5")

"""# AuTexTification dataset"""

!pip install datasets

from datasets import load_dataset

dataset = load_dataset("symanto/autextification2023", trust_remote_code=True)

print("Dataset structure:")
print(dataset)

from huggingface_hub import snapshot_download
import os

# Download entire repository
repo_path = snapshot_download(
    repo_id="symanto/autextification2023",
    repo_type="dataset",
    local_dir="./autextification_data"
)

print(f"Repository downloaded to: {repo_path}")

# List downloaded files
import os
for root, dirs, files in os.walk(repo_path):
    for file in files:
        if file.endswith(('.csv', '.parquet', '.jsonl', '.txt')):
            print(os.path.join(root, file))

